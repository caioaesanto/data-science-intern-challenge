{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas que necessitam ser instaladas\n",
    "# pip install perlin-noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b67dfb-b386-4b69-ba72-0e37f037fbd3",
   "metadata": {},
   "source": [
    "## Enunciado\n",
    "\n",
    "Neste notebook temos um problema clássico de Ciência de Dados, precisamos descobrir o preço do milho em um conjunto de cidades do Brasil. Contudo temos um problema sério, a única informação que temos hoje é a posição da cidade e o preço histórico dela, nossa modelagem não poderá se utilizar do preço atual, você ficará livre para propor fontes de informações que possam ser úteis na descoberta do preço do milho.\n",
    "\n",
    "O notebook já está estruturado no formato padrão, algumas células estarão vazias e com um comentário do é esperado que seja feito naquele trecho. \n",
    "\n",
    "Você tem total liberdade de adicionar nova células e criar novas ideias para o seu código, peço somente que mantenha as partes do código que já estão escritas pois elas servem para guiar o formato final do notebook\n",
    "\n",
    "Não se preocupe, caso alguma parte não funciona, pode adicionar um comentário ou alguma observação de qual foi o problema enferentado, mas lembre-se de sempre detalhar o seu pensamento, ele vale mais do que uma sintaxe errada no código!\n",
    "\n",
    "Boa Sorte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be4c7b-1c8b-42b2-854b-c5b6b3922cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Utilizadas durante o dercorrer normal da solução discutida\n",
    "import math\n",
    "import datetime as dt\n",
    "\n",
    "# Utilizadas durante a discussão sobre o modelo utilizado para gerar os dados para o desafio\n",
    "from matplotlib import pyplot as plt\n",
    "from perlin_noise import PerlinNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20baf2c9-2cbb-4aa8-b5bd-dfa0a68d3e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Leitura dos dados originais\n",
    "\n",
    "df = pd.read_csv('Data/real_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b506a-869f-4455-ba47-2ab08fc21435",
   "metadata": {},
   "source": [
    "No primeiro momento nós precisamos definir quais serão as novas fontes de dados, para isso fica a cargo de você escolher se utilizará outras fontes de dados, ou se podemos seguir somente com as iniciais.\n",
    "\n",
    "Caso decida por adicionar novas informações basta seguir o código a seguir que demonstra como adicionar novas informação ao DF original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ec027-f916-40c1-bffb-b1856fec6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('Data/cities_location.csv')\n",
    "\n",
    "df = df.merge(locations, left_on=['city_id'], right_on=['id'], how='left')\n",
    "df = df.drop(columns=['id'])\n",
    "\n",
    "# Caso tenha outras fontes de dados basta seguir o padrão\n",
    "# nova_fonte = pd.read_csv('arquivo')\n",
    "# df = df.merge(nova_fonte, left_on=['chave_no_df'], right_on=['chave_na_nova_fonte'], how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d5a25-5f47-43ac-bab0-6a88c180aaa8",
   "metadata": {},
   "source": [
    "Precisamos dividir o DF entre o que será meu treino e o que será meu teste, assim cabe a você definir como dividir esse dado entre treino e teste, qual o conjunto será utilizado para treino e qual será utilizado para teste (lembrando que a mesma cidade aparece várias vezes, já que temos vários dias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c91903-146a-42ec-a199-0c5336a68e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeando o índice pois causava um erro na biblioteca lightgbm devido a um caractere não permitido em linguagem JSON, o ':'\n",
    "df = df.rename(columns = {'Unnamed: 0' : 'index'})\n",
    "data_df = df.copy()\n",
    "\n",
    "# Dividindo a massa de dados na metade, utilizando a ordem original\n",
    "# a primeira metade seria para treinamento e a segunda metade para validação\n",
    "\n",
    "df_length = data_df.shape[0]\n",
    "df_train = data_df.head(math.floor(df_length/2))\n",
    "df_test = data_df.tail(math.floor(df_length/2))\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaafc3ed-5f9a-475b-911d-2117e9019f7d",
   "metadata": {},
   "source": [
    "Por fim separamos o target do conjunto de entrada, para isso basta selecionar a coluna que servirá de target no Y e remover a mesma do X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93db37a-0723-4298-bf3e-237a77b8b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloque o nome da coluna na variável abaixo\n",
    "\n",
    "target_col = 'price_per_unit'\n",
    "y = df_train[target_col]\n",
    "X = df_train.drop(['city_id', 'reference_date', target_col], axis=1)\n",
    "\n",
    "# Cria modelo\n",
    "lgbm_model = lgb.LGBMRegressor()\n",
    "lgbm_model.fit(X, y)\n",
    "\n",
    "# Predição\n",
    "y_test = df_test[target_col]\n",
    "X_test = df_test.drop(['city_id', 'reference_date', target_col], axis=1)\n",
    "predicted = lgbm_model.predict(X_test)\n",
    "\n",
    "# Calcula erro\n",
    "lgbm_mae  = mean_absolute_error(y_test, predicted)\n",
    "lgbm_rmse = mean_squared_error(y_test, predicted, squared=False)\n",
    "\n",
    "print(\"MAE =\", lgbm_mae)\n",
    "print(\"RMSE =\", lgbm_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ce9da",
   "metadata": {},
   "source": [
    "Aqui terminaria o algoritmo original, onde acabou-se por pegar os dados dos primeiros 86 dias para o treino do modelo e foi realizada a predição dos 130 dias restantes. O resultado do erro não foi satisfatório, uma vez que durante a conversa o Lucas Borges comentou que o erro obtido teria sido de apenas '0.5'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e96a0b-074f-4379-bbeb-fad1e0c07205",
   "metadata": {},
   "source": [
    "Fique a vontade para adicionar novas células com mais conteúdos da sua preferência a partir daqui, caso tenha algum insight interessante que ficaria fora da estrutura original basta adicionar e comentar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4de9a",
   "metadata": {},
   "source": [
    "Aqui começa uma experimentação para melhorar o desempenho da modelagem e predição. Primeiramente foi notado que uma das entradas do modelo é o índice do dado que tem uma função de transmitir o instante de um dado ao modelo. No entanto este índice não corresponde adequadamente a temporalidade dos dados, exemplificado na disparidade de dias que compreendem a primeira e segunda metade da massa de dados, 86 dias contra 130 dias. Foi então substituído o índice como entrada pela própria data de referência, após esta ter sido convertida em dias decorridos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df.copy()\n",
    "\n",
    "# Transformação do dado em formato de data para ordinal representando os dias.\n",
    "data_df['reference_date'] = pd.to_datetime(data_df['reference_date'])\n",
    "data_df['reference_date']= data_df['reference_date'].map(dt.datetime.toordinal)\n",
    "data_df['reference_date']= data_df['reference_date'].map(lambda x: x-data_df.loc[1,'reference_date'])\n",
    "\n",
    "# Dividindo a massa de dados na metade, utilizando a ordem original\n",
    "# a primeira metade seria para treinamento e a segunda metade para validação\n",
    "df_length = data_df.shape[0]\n",
    "df_train = data_df.head(math.floor(df_length/2))\n",
    "df_test = data_df.tail(math.floor(df_length/2))\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d476a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloque o nome da coluna na variável abaixo\n",
    "\n",
    "target_col = 'price_per_unit'\n",
    "y = df_train[target_col]\n",
    "X = df_train.drop(['city_id', 'index', target_col], axis=1)\n",
    "\n",
    "# Cria modelo\n",
    "lgbm_model = lgb.LGBMRegressor()\n",
    "lgbm_model.fit(X, y)\n",
    "\n",
    "# Predição\n",
    "y_test = df_test[target_col]\n",
    "X_test = df_test.drop(['city_id', 'index', target_col], axis=1)\n",
    "predicted = lgbm_model.predict(X_test)\n",
    "\n",
    "# Calcula erro\n",
    "lgbm_mae  = mean_absolute_error(y_test, predicted)\n",
    "lgbm_rmse = mean_squared_error(y_test, predicted, squared=False)\n",
    "\n",
    "print(\"MAE =\", lgbm_mae)\n",
    "print(\"RMSE =\", lgbm_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b3a57",
   "metadata": {},
   "source": [
    "O ganho de desempenho foi marginal, mas era esperado que não fosse uma melhora significativa, uma vez que o índice já passava a noção de temporalidade e esta foi apenas retocada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c866e",
   "metadata": {},
   "source": [
    "Na conversa foi comentado que este problema pode enfrentar diferentes cenários, isto é, a necessidade de modelagem e predição de preços pode advir tanto da falta de informação temporal quanto geográfica. O cenário anterior tinha acesso ao preço de todas as cidades, mas era limitado no tempo e buscava-se predizer o preço futuro; Já neste próximo cenário busca-se restringir a informação dos preços a metade das cidades e estimar o preço nas cidades faltantes. Para tal foi reordenada a massa de dados pelo identificador municipal, de modo a agrupar os dados de cada cidade para dividir as cidades que temos acesso a informação e as cidades que desejamos estimar o preço.\n",
    "\n",
    "Foi assumido que não existia forte correlação entre o 'city_id' e as latitudes e longitudes, isto é, assumiu-se que ambos os grupos de teste e treinamento tem cidades difusas em todo o espaço geográfico pesquisado. O que sendo inverdade causaria uma distorção, por exemplo se o grupo de treinamento fossem cidades do norte e o grupo de teste fossem cidades do sul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7547387",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df.copy()\n",
    "\n",
    "# Transformação do dado em formato de data para ordinal representando os dias.\n",
    "data_df['reference_date'] = pd.to_datetime(data_df['reference_date'])\n",
    "data_df['reference_date']= data_df['reference_date'].map(dt.datetime.toordinal)\n",
    "data_df['reference_date']= data_df['reference_date'].map(lambda x: x-737907)\n",
    "\n",
    "data_df = data_df.sort_values(by=['city_id','index'])\n",
    "data_df.head()\n",
    "\n",
    "# Dividindo a massa de dados na metade, utilizando a ordem original\n",
    "# a primeira metade seria para treinamento e a segunda metade para validação\n",
    "df_length = data_df.shape[0]\n",
    "df_train = data_df.head(math.floor(df_length/2))\n",
    "df_test = data_df.tail(math.floor(df_length/2))\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloque o nome da coluna na variável abaixo\n",
    "\n",
    "target_col = 'price_per_unit'\n",
    "y = df_train[target_col]\n",
    "X = df_train.drop(['city_id', 'index', target_col], axis=1)\n",
    "\n",
    "# Cria modelo\n",
    "lgbm_model = lgb.LGBMRegressor()\n",
    "lgbm_model.fit(X, y)\n",
    "\n",
    "# Predição\n",
    "y_test = df_test[target_col]\n",
    "X_test = df_test.drop(['city_id', 'index', target_col], axis=1)\n",
    "predicted = lgbm_model.predict(X_test)\n",
    "\n",
    "# Calcula erro\n",
    "lgbm_mae  = mean_absolute_error(y_test, predicted)\n",
    "lgbm_rmse = mean_squared_error(y_test, predicted, squared=False)\n",
    "\n",
    "print(\"MAE =\", lgbm_mae)\n",
    "print(\"RMSE =\", lgbm_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c978be5",
   "metadata": {},
   "source": [
    "O resultado foi extremamente próximo ao cenário anterior, sendo igualmente insatisfatório por ter uma média de erro elevada.\n",
    "\n",
    "Como foi discutido durante a conversa e apresentação do desafio, pensava em fazer uma modelagem mais próxima do que aprendi na faculdade, utilizando técnicas de modelagens de sistemas dinâmicos.\n",
    "\n",
    "Contudo ao se observar com mais calma a massa de dados ordenada por cidade e por data, podemos ver que a variação de preço entre um dia e outro na mesma cidade é enorme, o que levanta dúvidas quanto a real forma dessa massa de dados. A fim de comporeender melhor, foram desenhados gráficos de dispersão entre as entradas estudadas e a saída:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df.copy()\n",
    "\n",
    "data_df['reference_date'] = pd.to_datetime(data_df['reference_date'])\n",
    "data_df['reference_date']= data_df['reference_date'].map(dt.datetime.toordinal)\n",
    "data_df['reference_date']= data_df['reference_date'].map(lambda x: x-737907)\n",
    "\n",
    "data_df.plot.scatter(x='reference_date', y='price_per_unit', c='DarkBlue')\n",
    "data_df.plot.scatter(x='latitude', y='price_per_unit', c='DarkBlue')\n",
    "data_df.plot.scatter(x='longitude', y='price_per_unit', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c05fc9",
   "metadata": {},
   "source": [
    "Observando estes dados se esclarecem os resultados obtidos com as modelagens até então realizadas perante a verdade: A massa de dados é completamente aleatória! Tem características e aparenta ser nada mais que uma distribuição uniforme aleatória de preços, e para tal não há melhor predição que não a média de seus valores. Portanto entende-se que o que o Lucas Borges quis dizer com um resultado com uma diferença de apenas '0.5' seria entre a variação média da distribuição e os parâmetros de erro encontrados nas modelagens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512edfa",
   "metadata": {},
   "source": [
    "Ao meu entender para um desafio mais engajador, seria interessante que a massa de dados refletisse um modelo mais dinâmico e contínuo, como entendo intuitivamente ser a realidade. Para tal, poderiam ser empregadas ferramentas como o Perlin Noise, um tipo de ruído aleatório que não é descontínuo, de forma a representar por exemplo a relação geográfica de preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccdfb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de Perlin Noise\n",
    "noise = PerlinNoise(octaves=2, seed=1)\n",
    "xpix, ypix = 100, 100\n",
    "pic = [[noise([i/xpix, j/ypix]) for j in range(xpix)] for i in range(ypix)]\n",
    "\n",
    "plt.imshow(pic, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbb3e9",
   "metadata": {},
   "source": [
    "A figura acima gerada pelo Perlin Noise poderia representar um offset nos preços do milho baseada na longitude e latitude, onde por exemplo os pontos mais escuros apresentariam preços mais caros devido ao frete mais dificultado por estradas ruins. Uma figura diferente poderia representar em seus pontos mais escuros uma maior volatilidade dos preços, por uma região onde a queda de chuvas é mais inconsistentes, refletindo na safra. Auxiliado talvez a um preço base que por sua vez segue um outro modelo dinâmico relativo ao tempo poderia trazer a massa de dados e este desafio a uma simulaçào mais próxima do mercado do milho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73292d4a",
   "metadata": {},
   "source": [
    "De qualquer forma, achei interessante o desafio apresentado que me levou a quebrar a cabeça num ramo que ainda sei pouco e procuro entender mais, a ciência de dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
